{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.html.widgets import *\n",
    "import pickle\n",
    "import h5py \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense, Lambda\n",
    "from keras.layers import ELU\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_PATH = '/kaggle/dev/speed-challenge-2017-data/data'\n",
    "TRAIN_VIDEO = os.path.join(DATA_PATH, 'train.mp4') #'drive.mp4' 'train.mp4'\n",
    "TEST_VIDEO = os.path.join(DATA_PATH, 'test.mp4')\n",
    "CLEAN_DATA_PATH = '/kaggle/dev/speed-challenge-2017-data/clean_data'\n",
    "CLEAN_IMGS_TRAIN = os.path.join(CLEAN_DATA_PATH, 'train_imgs') #train2_imgs train_imgs\n",
    "CLEAN_IMGS_TEST = os.path.join(CLEAN_DATA_PATH, 'test_imgs')\n",
    "ASSETS_PATH = '/kaggle/dev/jovan/speed-challenge-2017/assets'\n",
    "\n",
    "train_frames = 8616 #20400 #8616\n",
    "test_frames = 10798\n",
    "\n",
    "seeds = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144]\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 25 #100 #90 \n",
    "steps_per_epoch = 400\n",
    "\n",
    "# run specific constants\n",
    "model_name = 'nvidia' #nvidia2\n",
    "run_name = 'model={}-batch_size={}-num_epoch={}-steps_per_epoch={}'.format(model_name,\n",
    "                                                                          batch_size,\n",
    "                                                                          num_epochs,\n",
    "                                                                          steps_per_epoch)\n",
    "\n",
    "assets_filepath = os.path.join(ASSETS_PATH, 'model_assets' , run_name)\n",
    "weights_loc = os.path.join(assets_filepath,'weights.h5')\n",
    "history_loc=  os.path.join(assets_filepath,'history.p')\n",
    "tensorboard_loc = os.path.join(assets_filepath, run_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model=nvidia-batch_size=16-num_epoch=25-steps_per_epoch=400'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(os.path.join(CLEAN_DATA_PATH, 'train_meta.csv')) #train2_meta\n",
    "print('shape: ', train_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: there is a chance that points might appear again. as n\n",
    "\n",
    "def train_valid_split(dframe, seed_val):\n",
    "    \"\"\"\n",
    "    Randomly shuffle pairs of rows in the dataframe, separates train and validation data\n",
    "    generates a uniform random variable 0->9, gives 20% chance to append to valid data, otherwise train_data\n",
    "    return tuple (train_data, valid_data) dataframes\n",
    "    \"\"\"\n",
    "    train_data = pd.DataFrame()\n",
    "    valid_data = pd.DataFrame()\n",
    "    np.random.seed(seed_val)\n",
    "    for i in tqdm(range(len(dframe) - 1)):\n",
    "        idx1 = np.random.randint(len(dframe) - 1)\n",
    "        idx2 = idx1 + 1\n",
    "        \n",
    "        \n",
    "        row1 = dframe.iloc[[idx1]].reset_index()\n",
    "        row2 = dframe.iloc[[idx2]].reset_index()\n",
    "        \n",
    "        randInt = np.random.randint(9)\n",
    "        if 0 <= randInt <= 1:\n",
    "            valid_frames = [valid_data, row1, row2]\n",
    "            valid_data = pd.concat(valid_frames, axis = 0, join = 'outer', ignore_index=False)\n",
    "        if randInt >= 2:\n",
    "            train_frames = [train_data, row1, row2]\n",
    "            train_data = pd.concat(train_frames, axis = 0, join = 'outer', ignore_index=False)\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_valid_split(train_meta, seeds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(train_data.sort_values(['image_index'])[['image_index']], train_data.sort_values(['image_index'])[['speed']], 'ro')\n",
    "plt.plot(valid_data.sort_values(['image_index'])[['image_index']], valid_data.sort_values(['image_index'])[['speed']], 'go')\n",
    "plt.xlabel('image_index (or time since start)')\n",
    "plt.ylabel('speed')\n",
    "plt.title('Speed vs time')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print('----')\n",
    "print('valid_data: ', valid_data.shape)\n",
    "print('train_data: ', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_brightness(image, bright_factor):\n",
    "    \"\"\"\n",
    "    Augments the brightness of the image by multiplying the saturation by a uniform random variable\n",
    "    Input: image (RGB)\n",
    "    returns: image with brightness augmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    # perform brightness augmentation only on the second channel\n",
    "    hsv_image[:,:,2] = hsv_image[:,:,2] * bright_factor\n",
    "    \n",
    "    # change back to RGB\n",
    "    image_rgb = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "    return image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img=mpimg.imread(train_meta['image_path'][60])\n",
    "# print('original image shape:', img.shape)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# bright_factor = 0.7 + np.random.uniform()\n",
    "# print('bright_factor:', bright_factor)\n",
    "# print('brightned image')\n",
    "# img=mpimg.imread(train_meta['image_path'][60])\n",
    "# plt.imshow(change_brightness(img, bright_factor))\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opticalFlowDense(image_current, image_next):\n",
    "    \"\"\"\n",
    "    input: image_current, image_next (RGB images)\n",
    "    calculates optical flow magnitude and angle and places it into HSV image\n",
    "    * Set the saturation to the saturation value of image_next\n",
    "    * Set the hue to the angles returned from computing the flow params\n",
    "    * set the value to the magnitude returned from computing the flow params\n",
    "    * Convert from HSV to RGB and return RGB image with same size as original image\n",
    "    \"\"\"\n",
    "    gray_current = cv2.cvtColor(image_current, cv2.COLOR_RGB2GRAY)\n",
    "    gray_next = cv2.cvtColor(image_next, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    \n",
    "    hsv = np.zeros((66, 220, 3))\n",
    "    # set saturation\n",
    "    hsv[:,:,1] = cv2.cvtColor(image_next, cv2.COLOR_RGB2HSV)[:,:,1]\n",
    " \n",
    "    # Flow Parameters\n",
    "#     flow_mat = cv2.CV_32FC2\n",
    "    flow_mat = None\n",
    "    image_scale = 0.5\n",
    "    nb_images = 1\n",
    "    win_size = 15\n",
    "    nb_iterations = 2\n",
    "    deg_expansion = 5\n",
    "    STD = 1.3\n",
    "    extra = 0\n",
    "\n",
    "    # obtain dense optical flow paramters\n",
    "    flow = cv2.calcOpticalFlowFarneback(gray_current, gray_next,  \n",
    "                                        flow_mat, \n",
    "                                        image_scale, \n",
    "                                        nb_images, \n",
    "                                        win_size, \n",
    "                                        nb_iterations, \n",
    "                                        deg_expansion, \n",
    "                                        STD, \n",
    "                                        0)\n",
    "                                        \n",
    "        \n",
    "    # convert from cartesian to polar\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  \n",
    "        \n",
    "    # hue corresponds to direction\n",
    "    hsv[:,:,0] = ang * (180/ np.pi / 2)\n",
    "    \n",
    "    # value corresponds to magnitude\n",
    "    hsv[:,:,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    # convert HSV to float32's\n",
    "    hsv = np.asarray(hsv, dtype= np.float32)\n",
    "    rgb_flow = cv2.cvtColor(hsv,cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    \n",
    "    return rgb_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    preprocesses the image\n",
    "    \n",
    "    input: image (480 (y), 640 (x), 3) RGB\n",
    "    output: image (shape is (220, 66, 3) as RGB)\n",
    "    \n",
    "    This stuff is performed on my validation data and my training data\n",
    "    Process: \n",
    "             1) Cropping out black spots\n",
    "             3) resize to (220, 66, 3) if not done so already from perspective transform\n",
    "    \"\"\"\n",
    "    # Crop out sky (top) (100px) and black right part (-90px)\n",
    "    #image_cropped = image[100:440, :-90] # -> (380, 550, 3) #v2 for data\n",
    "    image_cropped = image[25:375, :] #v1 for data\n",
    "    \n",
    "    image = cv2.resize(image_cropped, (220, 66), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# def preprocess_image_2(image):\n",
    "#     \"\"\"\n",
    "#     preprocesses the image\n",
    "    \n",
    "#     input: image (480 (y), 640 (x), 3) RGB\n",
    "#     output: image (shape is (220, 66, 3) as RGB)\n",
    "    \n",
    "#     This stuff is performed on my validation data and my training data\n",
    "#     Process: \n",
    "#              1) Cropping out black spots\n",
    "#              3) resize to (220, 66, 3) if not done so already from perspective transform\n",
    "#     \"\"\"\n",
    "#     # Crop out sky (top) (100px) and black right part (-90px)\n",
    "#     image_cropped = image[30:400, :-90] # -> (380, 550, 3)\n",
    "    \n",
    "#     image = cv2.resize(image_cropped, (220, 66), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=mpimg.imread(train_meta['image_path'][10])\n",
    "print('original image shape:', img.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "post_img = preprocess_image(img)\n",
    "print('post prosessed image:', post_img.shape)\n",
    "plt.imshow(post_img)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_valid_from_path(image_path, speed):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = preprocess_image(img)\n",
    "    return img, speed\n",
    "\n",
    "def preprocess_image_from_path(image_path, speed, bright_factor):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = change_brightness(img, bright_factor)    \n",
    "    img = preprocess_image(img)\n",
    "    return img, speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(data, batch_size = 32):\n",
    "    image_batch = np.zeros((batch_size, 66, 220, 3)) # nvidia input params\n",
    "    label_batch = np.zeros((batch_size))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # generate a random index with a uniform random distribution from 1 to len - 1\n",
    "            idx = np.random.randint(1, len(data) - 1)\n",
    "            \n",
    "            # Generate a random bright factor to apply to both images\n",
    "            bright_factor = 0.2 + np.random.uniform()\n",
    "            \n",
    "            row_now = data.iloc[[idx]].reset_index()\n",
    "            row_prev = data.iloc[[idx - 1]].reset_index()\n",
    "            row_next = data.iloc[[idx + 1]].reset_index()\n",
    "            \n",
    "            # Find the 3 respective times to determine frame order (current -> next)\n",
    "            \n",
    "            time_now = row_now['image_index'].values[0]\n",
    "            time_prev = row_prev['image_index'].values[0]\n",
    "            time_next = row_next['image_index'].values[0]\n",
    "            \n",
    "            if abs(time_now - time_prev) == 1 and time_now > time_prev:\n",
    "                row1 = row_prev\n",
    "                row2 = row_now\n",
    "                \n",
    "            elif abs(time_next - time_now) == 1 and time_next > time_now:\n",
    "                row1 = row_now\n",
    "                row2 = row_next\n",
    "            else:\n",
    "                print('Error generating row')            \n",
    "            \n",
    "            x1, y1 = preprocess_image_from_path(row1['image_path'].values[0],\n",
    "                                                row1['speed'].values[0],\n",
    "                                               bright_factor)\n",
    "            \n",
    "            # preprocess another image\n",
    "            x2, y2 = preprocess_image_from_path(row2['image_path'].values[0], \n",
    "                                                row2['speed'].values[0],\n",
    "                                               bright_factor)\n",
    "           \n",
    "            # compute optical flow send in images as RGB\n",
    "            rgb_diff = opticalFlowDense(x1, x2)\n",
    "                        \n",
    "            # calculate mean speed\n",
    "            y = np.mean([y1, y2])\n",
    "            \n",
    "            image_batch[i] = rgb_diff\n",
    "            label_batch[i] = y\n",
    "        \n",
    "        #print('image_batch', image_batch.shape, ' label_batch', label_batch)\n",
    "        # Shuffle the pairs before they get fed into the network\n",
    "        yield shuffle(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_data(data):\n",
    "    while True:\n",
    "        for idx in range(1, len(data) - 1): # start from the second row because we may try to grab it and need its prev to be in bounds\n",
    "            row_now = data.iloc[[idx]].reset_index()\n",
    "            row_prev = data.iloc[[idx - 1]].reset_index()\n",
    "            row_next = data.iloc[[idx + 1]].reset_index()\n",
    "            \n",
    "            # Find the 3 respective times to determine frame order (current -> next)\n",
    "            \n",
    "            time_now = row_now['image_index'].values[0]\n",
    "            time_prev = row_prev['image_index'].values[0]\n",
    "            time_next = row_next['image_index'].values[0]\n",
    "            \n",
    "            if abs(time_now - time_prev) == 1 and time_now > time_prev:\n",
    "                row1 = row_prev\n",
    "                row2 = row_now\n",
    "                \n",
    "            elif abs(time_next - time_now) == 1 and time_next > time_now:\n",
    "                row1 = row_now\n",
    "                row2 = row_next\n",
    "            else:\n",
    "                print('Error generating row')        \n",
    "            \n",
    "            x1, y1 = preprocess_image_valid_from_path(row1['image_path'].values[0], row1['speed'].values[0])\n",
    "            x2, y2 = preprocess_image_valid_from_path(row2['image_path'].values[0], row2['speed'].values[0])\n",
    "            \n",
    "            img_diff = opticalFlowDense(x1, x2)\n",
    "            img_diff = img_diff.reshape(1, img_diff.shape[0], img_diff.shape[1], img_diff.shape[2])\n",
    "            y = np.mean([y1, y2])\n",
    "            \n",
    "            speed = np.array([[y]])\n",
    "            \n",
    "            #print('img_diff', img_diff.shape, ' speed', speed)\n",
    "            yield img_diff, speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_img_height = 66\n",
    "N_img_width = 220\n",
    "N_img_channels = 3\n",
    "def nvidia_model():\n",
    "    inputShape = (N_img_height, N_img_width, N_img_channels)\n",
    "\n",
    "    model = Sequential()\n",
    "    # normalization    \n",
    "    # perform custom normalization before lambda layer in network\n",
    "    model.add(Lambda(lambda x: x/ 127.5 - 1, input_shape = inputShape))\n",
    "\n",
    "    model.add(Convolution2D(24, (5, 5), \n",
    "                            strides=(2,2), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv1'))\n",
    "    \n",
    "    \n",
    "    model.add(ELU())    \n",
    "    model.add(Convolution2D(36, (5, 5), \n",
    "                            strides=(2,2), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv2'))\n",
    "    \n",
    "    model.add(ELU())    \n",
    "    model.add(Convolution2D(48, (5, 5), \n",
    "                            strides=(2,2), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv3'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(64, (3, 3), \n",
    "                            strides = (1,1), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv4'))\n",
    "    \n",
    "    model.add(ELU())              \n",
    "    model.add(Convolution2D(64, (3, 3), \n",
    "                            strides= (1,1), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv5'))\n",
    "              \n",
    "              \n",
    "    model.add(Flatten(name = 'flatten'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(100, kernel_initializer = 'he_normal', name = 'fc1'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal', name = 'fc2'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(10, kernel_initializer = 'he_normal', name = 'fc3'))\n",
    "    model.add(ELU())\n",
    "    \n",
    "    # do not put activation at the end because we want to exact output, not a class identifier\n",
    "    model.add(Dense(1, name = 'output', kernel_initializer = 'he_normal'))\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer = adam, loss = 'mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(assets_filepath):\n",
    "    os.makedirs(assets_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator = generate_validation_data(valid_data)\n",
    "val_size = len(valid_data.index)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience=1, \n",
    "                              verbose=1, \n",
    "                              min_delta = 0.23,\n",
    "                              mode='min',)\n",
    "\n",
    "modelCheckpoint = ModelCheckpoint(weights_loc, \n",
    "                                  monitor = 'val_loss', \n",
    "                                  save_best_only = True, \n",
    "                                  mode = 'min', \n",
    "                                  verbose = 1,\n",
    "                                 save_weights_only = True)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_loc, histogram_freq=0,\n",
    "                            write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [modelCheckpoint, tensorboard, earlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nvidia_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_data.index)\n",
    "train_generator = generate_training_data(train_data, batch_size)\n",
    "history = model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch = steps_per_epoch, \n",
    "        epochs = num_epochs,\n",
    "        callbacks = callbacks_list,\n",
    "        verbose = 1,\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = val_size)\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving history\n",
    "pickle.dump(history.history, open(history_loc, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model processing hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 25 #100 #90 \n",
    "steps_per_epoch = 400\n",
    "\n",
    "# run specific constants\n",
    "model_name = 'nvidia' #nvidia2\n",
    "run_name = 'model={}-batch_size={}-num_epoch={}-steps_per_epoch={}'.format(model_name,\n",
    "                                                                          batch_size,\n",
    "                                                                          num_epochs,\n",
    "                                                                          steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pickle.load(open(history_loc, \"rb\" ))\n",
    "model.load_weights(weights_loc)\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = adam, loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_score = model.evaluate_generator(valid_generator, steps=val_size)\n",
    "print('val score:', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the training and validation loss for each epoch\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(history['loss'], 'ro--')\n",
    "plt.plot(history['val_loss'], 'go--')\n",
    "plt.title('NVIDIA mean squared error loss per epochs')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(data):\n",
    "    for idx in tqdm(range(1, len(data.index)-1)):\n",
    "        row_now = data.iloc[[idx]].reset_index()\n",
    "        row_prev = data.iloc[[idx - 1]].reset_index()\n",
    "        row_next = data.iloc[[idx + 1]].reset_index()\n",
    "        \n",
    "        time_now = row_now['image_index'].values[0]\n",
    "        time_prev = row_prev['image_index'].values[0]\n",
    "        time_next = row_next['image_index'].values[0]\n",
    "        \n",
    "        if abs(time_now - time_prev) == 1 and time_now > time_prev:\n",
    "            row1 = row_prev\n",
    "            row2 = row_now\n",
    "        elif abs(time_next - time_now) == 1 and time_next > time_now:\n",
    "            row1 = row_now\n",
    "            row2 = row_next\n",
    "        else:\n",
    "            print('Error generating row')\n",
    "        \n",
    "        x1, y1 = preprocess_image_valid_from_path(row1['image_path'].values[0], row1['speed'].values[0])\n",
    "        x2, y2 = preprocess_image_valid_from_path(row2['image_path'].values[0], row2['speed'].values[0])\n",
    "        \n",
    "        img_diff = opticalFlowDense(x1, x2)\n",
    "        img_diff = img_diff.reshape(1, img_diff.shape[0], img_diff.shape[1], img_diff.shape[2])\n",
    "        y = np.mean([y1, y2])\n",
    "\n",
    "        prediction = model.predict(img_diff)\n",
    "        error = abs(prediction-y2)\n",
    "        \n",
    "        \n",
    "        #print(prediction, error, time_now)\n",
    "        data.loc[data['image_index']==time_now, 'predicted_speed'] = prediction[0][0]\n",
    "        data.loc[data['image_index']==time_now, 'error'] = error[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_predictions(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(valid_data.iloc[1:-1, 3], \n",
    "                         valid_data.iloc[1:-1, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(valid_data.sort_values(['image_index'])[['image_index']], \n",
    "         valid_data.sort_values(['image_index'])[['speed']], 'go')\n",
    "plt.plot(valid_data.sort_values(['image_index'])[['image_index']], \n",
    "         valid_data.sort_values(['image_index'])[['predicted_speed']], 'bx')\n",
    "plt.xlabel('image_index (or time since start)')\n",
    "plt.ylabel('speed')\n",
    "plt.title('Predicted vs actual on validation data')\n",
    "plt.legend(['validation speed', 'predicted speed'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "ax1.plot(valid_data.sort_values(['image_index'])[['image_index']], \n",
    "         valid_data.sort_values(['image_index'])[['speed']], 'go', alpha=0.1)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(valid_data.sort_values(['image_index'])[['image_index']], \n",
    "         valid_data.sort_values(['image_index'])[['error']], 'co')\n",
    "\n",
    "ax1.set_xlabel('image_index (or time since start)')\n",
    "ax1.set_ylabel('speed')\n",
    "ax2.set_ylabel('prediction error')\n",
    "plt.title('Predicted vs actual on validation data')\n",
    "plt.legend(['prediction error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(error_thresh):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    ax.plot(valid_data.sort_values(['image_index'])[['image_index']], \n",
    "             valid_data.sort_values(['image_index'])[['speed']], 'go', alpha=0.4)\n",
    "\n",
    "\n",
    "    ax.plot(valid_data[valid_data['error']>error_thresh].sort_values(['image_index'])[['image_index']], \n",
    "             valid_data[valid_data['error']>error_thresh].sort_values(['image_index'])[['speed']], 'rx')\n",
    "\n",
    "\n",
    "    ax.set_xlabel('image_index (or time since start)')\n",
    "    ax.set_ylabel('speed')\n",
    "    plt.title('Error analysis on validation data')\n",
    "    plt.legend(['Ground truth', 'Incorrect prediction'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "interact(error_analysis, error_thresh=(0,5, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predicting on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = pd.read_csv(os.path.join(CLEAN_DATA_PATH, 'test_meta.csv'))\n",
    "assert(test_meta.shape[0] == test_frames)\n",
    "assert(test_meta.shape[1] == 3)\n",
    "print('shape: ', test_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25\n",
    "test_meta['smooth_predicted_speed'] = pd.rolling_median(test_meta['predicted_speed'], window_size, center=True)\n",
    "test_meta['smooth_error'] = test_meta.apply(lambda x: x['smooth_predicted_speed'] - x['speed'], axis=1)\n",
    "\n",
    "test_meta['smooth_predicted_speed'] = test_meta.apply(lambda x: \n",
    "                                                        x['predicted_speed'] if np.isnan(x['smooth_predicted_speed'])\n",
    "                                                       else x['smooth_predicted_speed'],axis=1)\n",
    "\n",
    "test_meta['smooth_error'] = test_meta.apply(lambda x: x['error'] if np.isnan(x['smooth_error'])\n",
    "                                                       else x['smooth_error'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(test_meta.sort_values(['image_index'])[['image_index']], \n",
    "         test_meta.sort_values(['image_index'])[['predicted_speed']], 'bx')\n",
    "plt.plot(test_meta.sort_values(['image_index'])[['image_index']], \n",
    "         test_meta.sort_values(['image_index'])[['smooth_predicted_speed']], 'g.')\n",
    "plt.xlabel('image_index (or time since start)')\n",
    "plt.ylabel('speed')\n",
    "plt.title('Predicted on test data')\n",
    "plt.legend(['test speed', 'predicted speed'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = test_meta['smooth_predicted_speed']\n",
    "output_file.to_csv(os.path.join(ASSETS_PATH, 'test.txt'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_predictions(train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25\n",
    "train_meta['smooth_predicted_speed'] = pd.rolling_median(train_meta['predicted_speed'], window_size, center=True)\n",
    "train_meta['smooth_error'] = train_meta.apply(lambda x: x['smooth_predicted_speed'] - x['speed'], axis=1)\n",
    "\n",
    "train_meta['smooth_predicted_speed'] = train_meta.apply(lambda x: \n",
    "                                                        x['predicted_speed'] if np.isnan(x['smooth_predicted_speed'])\n",
    "                                                       else x['smooth_predicted_speed'],axis=1)\n",
    "\n",
    "train_meta['smooth_error'] = train_meta.apply(lambda x: x['error'] if np.isnan(x['smooth_error'])\n",
    "                                                       else x['smooth_error'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(train_meta.sort_values(['image_index'])[['image_index']], \n",
    "         train_meta.sort_values(['image_index'])[['predicted_speed']], 'bx')\n",
    "plt.plot(train_meta.sort_values(['image_index'])[['image_index']], \n",
    "         train_meta.sort_values(['image_index'])[['smooth_predicted_speed']], 'g.')\n",
    "plt.plot(train_meta.sort_values(['image_index'])[['image_index']], \n",
    "         train_meta.sort_values(['image_index'])[['speed']], 'r.')\n",
    "plt.xlabel('image_index (or time since start)')\n",
    "plt.ylabel('speed')\n",
    "plt.title('Predicted vs actual on train data')\n",
    "plt.legend(['predicted speed', (str(window_size) +' window average'), 'ground truth'], loc='upper right')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(train_meta.iloc[1:-1, 2], \n",
    "                         train_meta.iloc[1:-1, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
