{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, ELU\n",
    "from keras.layers import Lambda, Dense, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import ELU\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline\n",
    "# np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 300, 3) 120 300 3\n"
     ]
    }
   ],
   "source": [
    "# Global Vars\n",
    "img = (cv.imread('../train/photos/images/gray_Road_Only/0.jpg')).shape\n",
    "img_height = img[0]\n",
    "img_width = img[1]\n",
    "img_channels = img[2]\n",
    "print(img, img_height, img_width, img_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_Value(prvs_FRAME, curr_FRAME, HSV_value):\n",
    "    prvs_FRAME = cv.cvtColor(prvs_FRAME, cv.COLOR_BGR2HSV)\n",
    "    curr_FRAME = cv.cvtColor(curr_FRAME, cv.COLOR_BGR2HSV)\n",
    "    prvs_FRAME[...,2] = prvs_FRAME[...,2]*HSV_value\n",
    "    curr_FRAME[...,2] = curr_FRAME[...,2]*HSV_value\n",
    "    return prvs_FRAME, curr_FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opticalFlow_DENSE(prvs_FRAME, curr_FRAME, dynamic_sat):\n",
    "    # flow_mat = None\n",
    "    # image_scale = 0.5\n",
    "    # pyr_images = 1 # was 3\n",
    "    # win_size = 15\n",
    "    # pyr_iterations = 2 # was 3\n",
    "    # poly_expans = 5\n",
    "    # std = 1.3 # was 1.2\n",
    "    \n",
    "    flow_mat = None\n",
    "    image_scale = 0.5\n",
    "    pyr_images = 1 # was 3\n",
    "    win_size = 15\n",
    "    pyr_iterations = 2 # was 3\n",
    "    poly_expans = 5\n",
    "    std = 1.3 # was 1.2\n",
    "\n",
    "    hsv = np.zeros_like(prvs_FRAME)\n",
    "    if dynamic_sat:\n",
    "        hsv[..., 1] = prvs_FRAME[..., 1]\n",
    "    else:\n",
    "        hsv[...,1] = 255\n",
    "    prvs_FRAME = cv.cvtColor(prvs_FRAME,cv.COLOR_BGR2GRAY)\n",
    "    curr_FRAME = cv.cvtColor(curr_FRAME,cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs_FRAME,curr_FRAME,flow_mat,image_scale,pyr_images,win_size,pyr_iterations,poly_expans,std,0)\n",
    "    mag, ang = cv.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv.normalize(mag,None,0,255,cv.NORM_MINMAX)\n",
    "    hsv = np.asarray(hsv, dtype=np.float32)\n",
    "    bgr = cv.cvtColor(hsv,cv.COLOR_HSV2BGR)\n",
    "    return bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(train_meta, kLoopCount, train_index, getSpeed, for_training):\n",
    "    global img_height, img_width, img_channels\n",
    "    opt_flows = np.empty((0, img_height, img_width, img_channels), dtype='float32')\n",
    "    speed = np.empty((0))\n",
    "    for k in range(kLoopCount):\n",
    "        HSV_value = np.random.uniform(low=.2, high=1.2)\n",
    "        #print(train_index)\n",
    "        curr_FRAME = cv.imread(train_meta['image_path'][train_index])\n",
    "        prvs_FRAME = cv.imread(train_meta['image_path'][train_index-1])\n",
    "        if getSpeed:\n",
    "            speed1 = train_meta['speed'][train_index]\n",
    "            speed2 = train_meta['speed'][train_index-1]\n",
    "            speed3 = np.mean((speed1, speed2))\n",
    "            speed = np.append(speed, speed3)\n",
    "        if for_training:\n",
    "            prvs_FRAME, curr_FRAME = adjust_Value(prvs_FRAME, curr_FRAME, HSV_value)\n",
    "        opt_flow = opticalFlow_DENSE(prvs_FRAME, curr_FRAME, False)\n",
    "        #opt_flow.dtype = 'uint8'\n",
    "        opt_flows = np.append(opt_flows, [opt_flow], axis=0)\n",
    "    if getSpeed:\n",
    "        return opt_flows, speed\n",
    "    else:\n",
    "        return opt_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(train_meta, train_index, batchSize):\n",
    "    global img_height, img_width, img_channels\n",
    "    get_speed = True\n",
    "    for_training = True\n",
    "    while True:\n",
    "        opt_flow_arr = np.empty((0, img_height, img_width, img_channels))\n",
    "        speed_arr = np.empty((0))\n",
    "        index2 = 0\n",
    "        for i in range(batchSize):\n",
    "            #print(index2)\n",
    "            index = np.random.randint(0, train_index.shape[0])\n",
    "            opt_flow, speed = getData(train_meta, 1, train_index[index], get_speed, for_training)\n",
    "            opt_flow_arr = np.append(opt_flow_arr, opt_flow, axis=0)\n",
    "            speed_arr = np.append(speed_arr, speed)\n",
    "            index2 = index2 + 1\n",
    "        #print(opt_flow_arr.shape)\n",
    "        yield opt_flow_arr, speed_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValData(train_meta, val_index, batchSize, getSpeed):\n",
    "    for_training = False\n",
    "    while True:\n",
    "        index = 0\n",
    "        for i in range(len(val_index)):\n",
    "            if getSpeed:\n",
    "                opt_flow, speed = getData(train_meta, 1, val_index[index], getSpeed, for_training)\n",
    "            else:\n",
    "                opt_flow = getData(train_meta, 1, val_index[index], getSpeed, for_training)\n",
    "            #opt_flow_arr = np.append(opt_flow_arr, opt_flow)\n",
    "            index = index + 1\n",
    "            if getSpeed:\n",
    "                yield opt_flow, speed\n",
    "            else:\n",
    "                yield opt_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCNNModel():\n",
    "    global img_height, img_width, img_channels\n",
    "#     model = Sequential()\n",
    "#     model.add(Lambda(lambda x: x / 127.5 - 1, input_shape=(img_height, img_width, img_channels)))\n",
    "#     model.add(Conv2D(filters=40, kernel_size=(5,5), strides=(3,3), activation='elu'))\n",
    "#     #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#     model.add(Conv2D(filters=20, kernel_size=(5,5), strides=(2,2), activation='elu'))\n",
    "#     #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#     #model.add(Dropout(rate=0.25))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(units=128, activation='elu'))\n",
    "#     #model.add(Dropout(rate=0.3))\n",
    "#     model.add(Dense(units=64, activation='elu'))\n",
    "#     model.add(Dense(units=15, activation='elu'))\n",
    "#     model.add(Dense(units=1, activation='elu'))\n",
    "#     opt = Adam(learning_rate=1e-4, epsilon=1e-08)\n",
    "#     model.compile(optimizer=opt, loss='mse')\n",
    "    \n",
    "    model = Sequential()\n",
    "    # normalization    \n",
    "    # perform custom normalization before lambda layer in network\n",
    "    model.add(Lambda(lambda x: x/ 127.5 - 1, input_shape = (img_height, img_width, img_channels)))\n",
    "\n",
    "    model.add(Conv2D(24, (5, 5), \n",
    "                     strides=(2,2), \n",
    "                     padding = 'valid',\n",
    "                     kernel_initializer = 'he_normal'))\n",
    "    \n",
    "    \n",
    "    model.add(ELU())    \n",
    "    model.add(Conv2D(36, (5, 5), \n",
    "                     strides=(2,2), \n",
    "                     padding = 'valid',\n",
    "                     kernel_initializer = 'he_normal'))\n",
    "    \n",
    "    model.add(ELU())    \n",
    "    model.add(Conv2D(48, (5, 5), \n",
    "                     strides=(2,2), \n",
    "                     padding = 'valid',\n",
    "                     kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(64, (3, 3), \n",
    "                     strides = (1,1), \n",
    "                     padding = 'valid',\n",
    "                     kernel_initializer = 'he_normal'))\n",
    "    \n",
    "    model.add(ELU())              \n",
    "    model.add(Conv2D(64, (3, 3), \n",
    "                     strides= (1,1), \n",
    "                     padding = 'valid',\n",
    "                     kernel_initializer = 'he_normal'))\n",
    "              \n",
    "              \n",
    "    model.add(Flatten())\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(100, kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(10, kernel_initializer = 'he_normal'))\n",
    "    model.add(ELU())\n",
    "    \n",
    "    # do not put activation at the end because we want to exact output, not a class identifier\n",
    "    model.add(Dense(1, name = 'output', kernel_initializer = 'he_normal'))\n",
    "    \n",
    "    # LR was 1e-4 and epsilon was 1e-08\n",
    "    adam = Adam(learning_rate=1e-3, epsilon=1e-07)\n",
    "    model.compile(optimizer = adam, loss = 'mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_print_fn(x):\n",
    "    global train_name\n",
    "    dic = {'lambd':'L', 'conv2':'C', 'flatt':'F', 'dense':'De', 'dropo':'D'}\n",
    "    dic2 = {'C':4, 'De':0}\n",
    "    dic3 = {',':0, ')':1, ' ':0}\n",
    "    if x[:5] in dic:\n",
    "        symbol = dic[x[:5]]\n",
    "        symbol_unit = ''\n",
    "        index = 13\n",
    "        commaSpace_count = 0\n",
    "        if symbol in dic2:\n",
    "            while True:\n",
    "                symbol_num = x[index]\n",
    "                if symbol_num not in dic3:\n",
    "                    if commaSpace_count == dic2[symbol]:\n",
    "                        symbol_unit = symbol_unit + symbol_num\n",
    "                    index = index + 1\n",
    "                else:\n",
    "                    if commaSpace_count == dic2[symbol]:\n",
    "                        break\n",
    "                    commaSpace_count = commaSpace_count+1\n",
    "                    index = index + 1\n",
    "        train_name = train_name + '-' + symbol + symbol_unit\n",
    "        print(train_name)\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=gray_Road_Only-elu-L\n",
      "model=gray_Road_Only-elu-L-C24\n",
      "model=gray_Road_Only-elu-L-C24-C36\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48-D\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48-D-C64\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50\n",
      "model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10\n",
      "../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/history.p \n",
      " ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5 \n",
      " ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gray_Road_Only'\n",
    "batch_size = 20\n",
    "epoch_num = 20\n",
    "steps_per_epoch = 400\n",
    "\n",
    "train_meta_loc = '../train/text/CSV'\n",
    "train_meta_loc = os.path.join(train_meta_loc, 'trainGrayRoadOnly_meta.csv')\n",
    "test_meta_loc = '../test/text/CSV'\n",
    "test_meta_loc = os.path.join(test_meta_loc, 'testGrayRoadOnly_meta.csv')\n",
    "\n",
    "asset_path = '../train/assets'\n",
    "#train_name = f'model={model_name}_C24_C36_C48_D50_C64_C64_De100_De50_De10_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}'\n",
    "#train_name = f'model={model_name}2_elu_C40P_C20P_D25_De128_D30_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}'\n",
    "#train_name = f'model={model_name}_elu_C40_C20_D25_De128_D30_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}__3'\n",
    "#train_name = f'model={model_name}_elu_C40_C20_D35_De128_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}__4'\n",
    "#train_name = f'model={model_name}_elu_C40_C20_De128_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}__2'\n",
    "train_name = f'model={model_name}-elu'\n",
    "model = createCNNModel()\n",
    "model.summary(line_length=100, positions=[0.06,0.55,0.67,0.0], print_fn=a_print_fn)\n",
    "model = 0\n",
    "train_name_end = f'-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}__2'\n",
    "train_name = train_name + train_name_end\n",
    "train_path = os.path.join(asset_path, train_name)\n",
    "\n",
    "model_train_meta_loc = os.path.join(train_path, 'train_meta.csv')\n",
    "model_test_meta_loc = os.path.join(train_path, 'test_meta.csv')\n",
    "test_result_path = os.path.join(train_path, 'test.txt')\n",
    "history_loc = os.path.join(train_path, 'history.p')\n",
    "weights_loc = os.path.join(train_path, 'weights.h5')\n",
    "tensorboard_loc = os.path.join(train_path, train_name)\n",
    "print(history_loc, '\\n', weights_loc, '\\n', tensorboard_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    global train_meta_loc, batch_size, epoch_num, steps_per_epoch, train_path\n",
    "    train_meta = pd.read_csv(train_meta_loc)\n",
    "    \n",
    "    data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "    dataset_frames_num = train_meta.shape[0]\n",
    "    train_frames_num = int(.78*(dataset_frames_num-1))\n",
    "    val_frames_num = int((dataset_frames_num - train_frames_num))\n",
    "    \n",
    "    \n",
    "    train_index = data_index[:train_frames_num]\n",
    "    val_index = data_index[train_frames_num:(train_frames_num+val_frames_num)]\n",
    "    print(train_index.shape, val_index.shape)\n",
    "    \n",
    "    if not os.path.exists(train_path):\n",
    "        os.makedirs(train_path)\n",
    "    \n",
    "    valid_generator = getValData(train_meta, val_index, batch_size, True)\n",
    "    val_size = len(val_index)\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                                  patience=1, \n",
    "                                  verbose=1, \n",
    "                                  min_delta = 0.23,\n",
    "                                  mode='min',)\n",
    "    \n",
    "    modelCheckpoint = ModelCheckpoint(weights_loc, \n",
    "                                      monitor = 'val_loss', \n",
    "                                      save_best_only = True, \n",
    "                                      mode = 'min', \n",
    "                                      verbose = 1,\n",
    "                                     save_weights_only = True)\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=tensorboard_loc, histogram_freq=0,\n",
    "                                write_graph=True, write_images=True)\n",
    "    #callbacks_list = [modelCheckpoint, tensorboard, earlyStopping]\n",
    "    callbacks_list = [modelCheckpoint, tensorboard]\n",
    "    \n",
    "    model = createCNNModel()\n",
    "    \n",
    "    train_size = len(train_index)\n",
    "    train_generator = getTrainingData(train_meta, train_index, batch_size)\n",
    "    history = model.fit_generator(\n",
    "            train_generator, \n",
    "            steps_per_epoch = steps_per_epoch, \n",
    "            epochs = epoch_num,\n",
    "            callbacks = callbacks_list,\n",
    "            verbose = 1,\n",
    "            validation_data = valid_generator,\n",
    "            validation_steps = val_size)\n",
    "\n",
    "    print(history)\n",
    "    pickle.dump(history.history, open(history_loc, \"wb\"))\n",
    "    \n",
    "    return model, history.history, valid_generator, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15911,) (4488,)\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 142s 356ms/step - loss: 72.9582 - val_loss: 296.2667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 296.26672, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 138s 345ms/step - loss: 30.1403 - val_loss: 216.1462\n",
      "\n",
      "Epoch 00002: val_loss improved from 296.26672 to 216.14621, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 138s 346ms/step - loss: 25.7041 - val_loss: 267.1067\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 216.14621\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 138s 345ms/step - loss: 23.5478 - val_loss: 165.6489\n",
      "\n",
      "Epoch 00004: val_loss improved from 216.14621 to 165.64890, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 138s 345ms/step - loss: 21.8140 - val_loss: 133.2873\n",
      "\n",
      "Epoch 00005: val_loss improved from 165.64890 to 133.28731, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 19.6409 - val_loss: 174.4023\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 133.28731\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 18.8040 - val_loss: 80.8824\n",
      "\n",
      "Epoch 00007: val_loss improved from 133.28731 to 80.88235, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 17.7591 - val_loss: 40.6523\n",
      "\n",
      "Epoch 00008: val_loss improved from 80.88235 to 40.65230, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 137s 342ms/step - loss: 15.7646 - val_loss: 59.9310\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 40.65230\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 22.7693 - val_loss: 319.6436\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 40.65230\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 137s 342ms/step - loss: 22.3256 - val_loss: 80.5998\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 40.65230\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 138s 344ms/step - loss: 20.3872 - val_loss: 71.3602\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 40.65230\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 20.9489 - val_loss: 8.2055\n",
      "\n",
      "Epoch 00013: val_loss improved from 40.65230 to 8.20550, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 20.0691 - val_loss: 9.5319\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 8.20550\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 136s 341ms/step - loss: 18.3339 - val_loss: 23.1586\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 8.20550\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 21.1649 - val_loss: 16.7829\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 8.20550\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 137s 342ms/step - loss: 19.7340 - val_loss: 10.6004\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 8.20550\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 19.9893 - val_loss: 0.5096\n",
      "\n",
      "Epoch 00018: val_loss improved from 8.20550 to 0.50963, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 137s 343ms/step - loss: 18.5740 - val_loss: 0.3553\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.50963 to 0.35528, saving model to ../train/assets/model=gray_Road_Only-elu-L-C24-C36-C48-D-C64-C64-F-De100-De50-De10-batch_size=20-num_epoch=20-steps_per_epoch=400__2/weights.h5\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 137s 342ms/step - loss: 18.9489 - val_loss: 0.5903\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.35528\n",
      "<keras.callbacks.callbacks.History object at 0x7fbf387ed460>\n"
     ]
    }
   ],
   "source": [
    "model, history, valid_generator, val_size = start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = pickle.load(open(history_loc, \"rb\" ))\n",
    "model = createCNNModel()\n",
    "model.load_weights(weights_loc)\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = adam, loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12239 4080 4080\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.read_csv(train_meta_loc)\n",
    "\n",
    "data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "dataset_frames_num = train_meta.shape[0]\n",
    "#train_frames_num = int(.8*(dataset_frames_num-1))\n",
    "train_frames_num = int(.6*(dataset_frames_num-1))\n",
    "val_frames_num = int(.5*((dataset_frames_num-1) - train_frames_num))\n",
    "test_frames_num = int(((dataset_frames_num-1) - (train_frames_num + val_frames_num)))\n",
    "print(train_frames_num, val_frames_num, test_frames_num)\n",
    "\n",
    "train_index = data_index[:train_frames_num]\n",
    "val_index = data_index[train_frames_num:(train_frames_num+val_frames_num)]\n",
    "test_index = data_index[(train_frames_num+val_frames_num):]\n",
    "train_generator = getValData(train_meta, train_index, batch_size, True)\n",
    "valid_generator = getValData(train_meta, val_index, batch_size, True)\n",
    "test_generator = getValData(train_meta, test_index, batch_size, True)\n",
    "train_size = len(train_index)\n",
    "val_size = len(val_index)\n",
    "test_size = len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12239/12239 [==============================] - 115s 9ms/step\n",
      "train score: 4.114180088043213\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(train_generator, steps=train_size)\n",
    "print('train score:', train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/4080 [==============================] - 39s 10ms/step\n",
      "val score: 27.780153274536133\n"
     ]
    }
   ],
   "source": [
    "val_score = model.evaluate(valid_generator, steps=val_size)\n",
    "print('val score:', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/4080 [==============================] - 38s 9ms/step\n",
      "test score: 0.2292296290397644\n"
     ]
    }
   ],
   "source": [
    "test_score = model.evaluate(test_generator, steps=test_size)\n",
    "print('test score:', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'], 'ro')\n",
    "plt.plot(history['val_loss'], 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size)\n",
    "fig_size[0] = 30\n",
    "fig_size[1] = 30\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "fig_size2 = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(train_meta_loc)\n",
    "\n",
    "data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "dataset_frames_num = train_meta.shape[0]\n",
    "train_frames_num = int(.70*(dataset_frames_num-1))\n",
    "\n",
    "train_index = data_index[:train_frames_num]\n",
    "val_index = data_index[train_frames_num:]\n",
    "print(train_index, val_index)\n",
    "plt.plot(train_meta['speed'][train_index], 'ro')\n",
    "plt.plot(train_meta['speed'][val_index], 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSpeed(a_meta, a_meta_loc, getSpeed, setError):\n",
    "    global model_train_meta_loc, model_test_meta_loc, batch_size, weights_loc\n",
    "\n",
    "    dataset_frames_num = a_meta.shape[0]\n",
    "    print(dataset_frames_num)\n",
    "    \n",
    "    data_index = np.arange(1, (dataset_frames_num))\n",
    "    data_length = data_index.shape[0]\n",
    "    \n",
    "    data_gen = getValData(a_meta, data_index, batch_size, getSpeed)\n",
    "    \n",
    "    model = createCNNModel()\n",
    "    model.load_weights(weights_loc)\n",
    "    predicted_speed = model.predict(data_gen, steps=data_length, verbose=1)\n",
    "    predicted_speed.shape = dataset_frames_num-1\n",
    "    \n",
    "    a_meta = a_meta.assign(predicted_speed=pd.Series(np.empty((dataset_frames_num))).values)\n",
    "    a_meta.loc[:, 'predicted_speed'] = np.nan\n",
    "    a_meta.loc[1:,'predicted_speed'] = predicted_speed\n",
    "    \n",
    "    if setError:\n",
    "        a_meta = a_meta.assign(error=pd.Series(np.empty((dataset_frames_num))).values)\n",
    "        a_meta.loc[:]['error'] = np.nan\n",
    "        a_meta.loc[1:,'error'] = abs(a_meta.loc[1:, 'speed'] - predicted_speed)\n",
    "\n",
    "    a_meta.to_csv(a_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSpeed(train_meta, model_train_meta_loc, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(model_train_meta_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'predicted_speed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(train_meta[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = train_meta.loc[:, :'error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_meta.loc[:,'error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_meta.loc[:,'med_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_meta.loc[:,'mean_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_meta.loc[20376:, 'predicted_speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 50 # was 50\n",
    "#train_meta = train_meta.assign(med_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:, 'med_prd_spd'] = np.nan\n",
    "train_meta.loc[1:,'med_prd_spd'] = train_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).median()\n",
    "train_meta.loc[1:25, 'med_prd_spd'] = train_meta.loc[1:25, 'predicted_speed']\n",
    "train_meta.loc[20376:, 'med_prd_spd'] = train_meta.loc[20376:, 'predicted_speed']\n",
    "#train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 50 # was 50\n",
    "#train_meta = train_meta.assign(mean_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:, 'mean_prd_spd'] = np.nan\n",
    "train_meta.loc[1:,'mean_prd_spd'] = train_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).mean()\n",
    "train_meta.loc[1:25, 'mean_prd_spd'] = train_meta.loc[1:25, 'predicted_speed']\n",
    "train_meta.loc[20376:, 'mean_prd_spd'] = train_meta.loc[20376:, 'predicted_speed']\n",
    "#train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta = train_meta.assign(med_error=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:]['med_error'] = np.nan\n",
    "train_meta.loc[1:,'med_error'] = abs(train_meta.loc[1:, 'speed'] - train_meta.loc[1:,'med_prd_spd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta = train_meta.assign(mean_error=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:]['mean_error'] = np.nan\n",
    "train_meta.loc[1:,'mean_error'] = abs(train_meta.loc[1:, 'speed'] - train_meta.loc[1:,'mean_prd_spd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(train_meta.loc[:,'error']))\n",
    "print(np.mean(train_meta.loc[:,'med_error']))\n",
    "print(np.mean(train_meta.loc[:,'mean_error']))\n",
    "print()\n",
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'predicted_speed']))\n",
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'med_prd_spd']))\n",
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'mean_prd_spd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "dataset_frames_num = train_meta.shape[0]\n",
    "#train_frames_num = int(.8*(dataset_frames_num-1))\n",
    "train_frames_num = int(.6*(dataset_frames_num-1))\n",
    "val_frames_num = int(.5*((dataset_frames_num-1) - train_frames_num))\n",
    "test_frames_num = int(((dataset_frames_num-1) - (train_frames_num + val_frames_num)))\n",
    "print(train_frames_num, val_frames_num, test_frames_num, train_frames_num+val_frames_num+test_frames_num)\n",
    "\n",
    "train_index = data_index[:train_frames_num]\n",
    "val_index = data_index[train_frames_num:(train_frames_num+val_frames_num)]\n",
    "test_index = data_index[(train_frames_num+val_frames_num):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = 0\n",
    "test2 = 0\n",
    "test3 = 0\n",
    "for i in train_index:\n",
    "    test1 = test1 + train_meta.loc[i, 'error']\n",
    "test1_loss = test1/train_frames_num\n",
    "for i in val_index:\n",
    "    test2 = test2 + train_meta.loc[i, 'error']\n",
    "test2_loss = test2/val_frames_num\n",
    "for i in test_index:\n",
    "    test3 = test3 + train_meta.loc[i, 'error']\n",
    "test3_loss = test3/test_frames_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test1_loss, test2_loss, test3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = pd.read_csv(test_meta_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSpeed(test_meta, model_test_meta_loc, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = pd.read_csv(model_test_meta_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(test_meta[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 100 # was 50\n",
    "#test_meta = test_meta.assign(med_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "test_meta.loc[:, 'med_prd_spd'] = np.nan\n",
    "test_meta.loc[1:,'med_prd_spd'] = test_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).median()\n",
    "test_meta.loc[1:25, 'med_prd_spd'] = test_meta.loc[1:25, 'predicted_speed']\n",
    "test_meta.loc[10774:, 'med_prd_spd'] = test_meta.loc[10774:, 'predicted_speed']\n",
    "#test_meta.to_csv(model_test_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 100 # was 50\n",
    "#test_meta = test_meta.assign(mean_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "test_meta.loc[:, 'mean_prd_spd'] = np.nan\n",
    "test_meta.loc[1:,'mean_prd_spd'] = test_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).mean()\n",
    "test_meta.loc[1:25, 'mean_prd_spd'] = test_meta.loc[1:25, 'predicted_speed']\n",
    "test_meta.loc[10774:, 'mean_prd_spd'] = test_meta.loc[10774:, 'predicted_speed']\n",
    "#test_meta.to_csv(model_test_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta.to_csv(model_test_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_length = test_meta.loc[1080:1640].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = 0\n",
    "test2 = 0\n",
    "test3 = 0\n",
    "for i in test_meta.loc[1080:1640, 'predicted_speed']:\n",
    "    test1 = test1 + i\n",
    "test1_loss = test1/stop_length\n",
    "for i in test_meta.loc[1080:1640, 'med_prd_spd']:\n",
    "    test2 = test2 + i\n",
    "test2_loss = test2/stop_length\n",
    "for i in test_meta.loc[1080:1640, 'mean_prd_spd']:\n",
    "    test3 = test3 + i\n",
    "test3_loss = test3/stop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test1_loss, test2_loss, test3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test_meta.loc[:, 'med_prd_spd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv(test_result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size)\n",
    "fig_size[0] = 15\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "fig_size2 = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'predicted_speed'], 'bx')\n",
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'speed'], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'med_prd_spd'], 'bx')\n",
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'speed'], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'mean_prd_spd'], 'bx')\n",
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'speed'], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_meta.loc[1:, 'image_index'], test_meta.loc[1:, 'predicted_speed'], 'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_meta.loc[1:, 'image_index'], test_meta.loc[1:, 'med_prd_spd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_meta.loc[1:, 'image_index'], test_meta.loc[1:, 'mean_prd_spd'], 'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
