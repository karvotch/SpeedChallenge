{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, ELU\n",
    "from keras.layers import Lambda, Dense, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import ELU\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline\n",
    "# np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 320, 3) 136 320 3\n"
     ]
    }
   ],
   "source": [
    "# Global Vars\n",
    "img = (cv.imread('../train/photos/images/gray_Clipped/0.jpg')).shape\n",
    "img_height = img[0]\n",
    "img_width = img[1]\n",
    "img_channels = img[2]\n",
    "print(img, img_height, img_width, img_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_Value(prvs_FRAME, curr_FRAME, HSV_value):\n",
    "    prvs_FRAME = cv.cvtColor(prvs_FRAME, cv.COLOR_BGR2HSV)\n",
    "    curr_FRAME = cv.cvtColor(curr_FRAME, cv.COLOR_BGR2HSV)\n",
    "    prvs_FRAME[...,2] = prvs_FRAME[...,2]*HSV_value\n",
    "    curr_FRAME[...,2] = curr_FRAME[...,2]*HSV_value\n",
    "    prvs_FRAME = cv.cvtColor(prvs_FRAME, cv.COLOR_HSV2BGR)\n",
    "    curr_FRAME = cv.cvtColor(curr_FRAME, cv.COLOR_HSV2BGR)\n",
    "    return prvs_FRAME, curr_FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opticalFlow_DENSE(prvs_FRAME, curr_FRAME, dynamic_sat):\n",
    "    # flow_mat = None\n",
    "    # image_scale = 0.5\n",
    "    # pyr_images = 1 # was 3\n",
    "    # win_size = 15\n",
    "    # pyr_iterations = 2 # was 3\n",
    "    # poly_expans = 5\n",
    "    # std = 1.3 # was 1.2\n",
    "    \n",
    "    flow_mat = None\n",
    "    image_scale = 0.5\n",
    "    pyr_images = 1 # was 3\n",
    "    win_size = 15\n",
    "    pyr_iterations = 2 # was 3\n",
    "    poly_expans = 5\n",
    "    std = 1.3 # was 1.2\n",
    "\n",
    "    hsv = np.zeros_like(prvs_FRAME)\n",
    "    if dynamic_sat:\n",
    "        hsv[..., 1] = cv.cvtColor(prvs_FRAME, cv.COLOR_BGR2HSV)[...,1]\n",
    "    else:\n",
    "        hsv[...,1] = 255\n",
    "    prvs_FRAME = cv.cvtColor(prvs_FRAME,cv.COLOR_BGR2GRAY)\n",
    "    curr_FRAME = cv.cvtColor(curr_FRAME,cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs_FRAME,curr_FRAME,flow_mat,image_scale,pyr_images,win_size,pyr_iterations,poly_expans,std,0)\n",
    "    mag, ang = cv.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*(180/np.pi/2)\n",
    "    hsv[...,2] = cv.normalize(mag,None,0,255,cv.NORM_MINMAX)\n",
    "#     hsv = np.asarray(hsv, dtype=np.float32)\n",
    "    bgr = cv.cvtColor(hsv,cv.COLOR_HSV2BGR)\n",
    "    return bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(train_meta, kLoopCount, train_index, getSpeed, for_training):\n",
    "    global img_height, img_width, img_channels\n",
    "    opt_flows = np.empty((0, img_height, img_width, img_channels), dtype='float32')\n",
    "    speed = np.empty((0))\n",
    "    for k in range(kLoopCount):\n",
    "        HSV_value = np.random.uniform(low=.5, high=1.2)\n",
    "        #print(train_index)\n",
    "        curr_FRAME = cv.imread(train_meta['image_path'][train_index])\n",
    "        prvs_FRAME = cv.imread(train_meta['image_path'][train_index-1])\n",
    "        if getSpeed:\n",
    "            speed1 = train_meta['speed'][train_index]\n",
    "            speed2 = train_meta['speed'][train_index-1]\n",
    "            speed3 = np.mean((speed1, speed2))\n",
    "            speed = np.append(speed, speed1)\n",
    "        if for_training:\n",
    "            prvs_FRAME, curr_FRAME = adjust_Value(prvs_FRAME, curr_FRAME, HSV_value)\n",
    "        opt_flow = opticalFlow_DENSE(prvs_FRAME, curr_FRAME, False)\n",
    "#         opt_flows = np.append(opt_flows, [opt_flow], axis=0)\n",
    "        opt_flows = np.append(opt_flows, [np.asarray(opt_flow, dtype=np.float32)], axis=0)\n",
    "    if getSpeed:\n",
    "        return opt_flows, speed\n",
    "    else:\n",
    "        return opt_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(train_meta, train_index, batchSize):\n",
    "    global img_height, img_width, img_channels\n",
    "    get_speed = True\n",
    "    for_training = False\n",
    "    while True:\n",
    "        opt_flow_arr = np.empty((0, img_height, img_width, img_channels))\n",
    "        speed_arr = np.empty((0))\n",
    "        for i in range(batchSize):\n",
    "            index = np.random.randint(0, train_index.shape[0])\n",
    "            opt_flow, speed = getData(train_meta, 1, train_index[index], get_speed, for_training)\n",
    "            opt_flow_arr = np.append(opt_flow_arr, opt_flow, axis=0)\n",
    "            speed_arr = np.append(speed_arr, speed)\n",
    "        #print(opt_flow_arr.shape)\n",
    "        yield opt_flow_arr, speed_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValData(train_meta, val_index, batchSize, getSpeed):\n",
    "    for_training = False\n",
    "    while True:\n",
    "        index = 0\n",
    "        for i in range(len(val_index)):\n",
    "            if getSpeed:\n",
    "                opt_flow, speed = getData(train_meta, 1, val_index[index], getSpeed, for_training)\n",
    "            else:\n",
    "                opt_flow = getData(train_meta, 1, val_index[index], getSpeed, for_training)\n",
    "            #opt_flow_arr = np.append(opt_flow_arr, opt_flow)\n",
    "            index = index + 1\n",
    "            if getSpeed:\n",
    "                yield opt_flow, speed\n",
    "            else:\n",
    "                yield opt_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCNNModel():\n",
    "    global img_height, img_width, img_channels\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x / 127.5 - 1, input_shape=(img_height, img_width, img_channels)))\n",
    "    model.add(Conv2D(filters=40, kernel_size=(5,5), strides=(2,2), activation='elu', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=60, kernel_size=(5,5), strides=(2,2), activation='elu', kernel_initializer = 'he_normal'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=128, activation='elu', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(units=64, activation='elu', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(units=32, activation='elu', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(units=16, activation='elu', kernel_initializer = 'he_normal'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(units=1, activation='elu', kernel_initializer = 'he_normal'))\n",
    "    opt = Adam(learning_rate=1e-3, epsilon=1e-07)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     # normalization    \n",
    "#     # perform custom normalization before lambda layer in network\n",
    "#     model.add(Lambda(lambda x: x/ 127.5 - 1, input_shape = (img_height, img_width, img_channels)))\n",
    "\n",
    "#     model.add(Conv2D(24, (5, 5), \n",
    "#                      strides=(2,2), \n",
    "#                      padding = 'valid',\n",
    "#                      kernel_initializer = 'he_normal'))\n",
    "    \n",
    "    \n",
    "#     model.add(ELU())    \n",
    "#     model.add(Conv2D(36, (5, 5), \n",
    "#                      strides=(2,2), \n",
    "#                      padding = 'valid',\n",
    "#                      kernel_initializer = 'he_normal'))\n",
    "    \n",
    "#     model.add(ELU())    \n",
    "#     model.add(Conv2D(48, (5, 5), \n",
    "#                      strides=(2,2), \n",
    "#                      padding = 'valid',\n",
    "#                      kernel_initializer = 'he_normal'))\n",
    "#     model.add(ELU())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Conv2D(64, (3, 3), \n",
    "#                      strides = (1,1), \n",
    "#                      padding = 'valid',\n",
    "#                      kernel_initializer = 'he_normal'))\n",
    "    \n",
    "#     model.add(ELU())              \n",
    "#     model.add(Conv2D(64, (3, 3), \n",
    "#                      strides= (1,1), \n",
    "#                      padding = 'valid',\n",
    "#                      kernel_initializer = 'he_normal'))\n",
    "              \n",
    "              \n",
    "#     model.add(Flatten())\n",
    "#     model.add(ELU())\n",
    "#     model.add(Dense(100, kernel_initializer = 'he_normal'))\n",
    "#     model.add(ELU())\n",
    "#     model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "#     model.add(ELU())\n",
    "#     model.add(Dense(10, kernel_initializer = 'he_normal'))\n",
    "#     model.add(ELU())\n",
    "    \n",
    "#     # do not put activation at the end because we want to exact output, not a class identifier\n",
    "#     model.add(Dense(1, kernel_initializer = 'he_normal'))\n",
    "    \n",
    "#     # LR was 1e-4 and epsilon was 1e-08\n",
    "#     adam = Adam(learning_rate=1e-3, epsilon=1e-08)\n",
    "#     model.compile(optimizer = adam, loss = 'mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_print_fn(x):\n",
    "    global train_name\n",
    "    dic = {'lambd':'L', 'conv2':'C', 'flatt':'F', 'dense':'De', 'dropo':'D'}\n",
    "    dic2 = {'C':4, 'De':0}\n",
    "    dic3 = {',':0, ')':1, ' ':0}\n",
    "    if x[:5] in dic:\n",
    "        symbol = dic[x[:5]]\n",
    "        symbol_unit = ''\n",
    "        index = 13\n",
    "        commaSpace_count = 0\n",
    "        if symbol in dic2:\n",
    "            while True:\n",
    "                symbol_num = x[index]\n",
    "                if symbol_num not in dic3:\n",
    "                    if commaSpace_count == dic2[symbol]:\n",
    "                        symbol_unit = symbol_unit + symbol_num\n",
    "                    index = index + 1\n",
    "                else:\n",
    "                    if commaSpace_count == dic2[symbol]:\n",
    "                        break\n",
    "                    commaSpace_count = commaSpace_count+1\n",
    "                    index = index + 1\n",
    "        train_name = train_name + '-' + symbol + symbol_unit\n",
    "#         print(train_name)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "____________________________________________________________________________________________________\n",
      "Layer Output Shape                                     Param #     \n",
      "====================================================================================================\n",
      "lambd (None, 136, 320, 3)                              0           \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (None, 66, 158, 40)                              3040        \n",
      "____________________________________________________________________________________________________\n",
      "dropo (None, 66, 158, 40)                              0           \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (None, 31, 77, 60)                               60060       \n",
      "____________________________________________________________________________________________________\n",
      "dropo (None, 31, 77, 60)                               0           \n",
      "____________________________________________________________________________________________________\n",
      "flatt (None, 143220)                                   0           \n",
      "____________________________________________________________________________________________________\n",
      "dense (None, 128)                                      18332288    \n",
      "____________________________________________________________________________________________________\n",
      "dropo (None, 128)                                      0           \n",
      "____________________________________________________________________________________________________\n",
      "dense (None, 64)                                       8256        \n",
      "____________________________________________________________________________________________________\n",
      "dropo (None, 64)                                       0           \n",
      "____________________________________________________________________________________________________\n",
      "dense (None, 32)                                       2080        \n",
      "____________________________________________________________________________________________________\n",
      "dropo (None, 32)                                       0           \n",
      "____________________________________________________________________________________________________\n",
      "dense (None, 16)                                       528         \n",
      "____________________________________________________________________________________________________\n",
      "dropo (None, 16)                                       0           \n",
      "____________________________________________________________________________________________________\n",
      "dense (None, 1)                                        17          \n",
      "====================================================================================================\n",
      "Total params: 18,406,269\n",
      "Trainable params: 18,406,269\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "../train/assets/model=gray_Clipped-elu-L-C40-D-C60-D-F-De128-D-De64-D-De32-D-De16-D-De1-batch_size=40-num_epoch=20-steps_per_epoch=400/history.p \n",
      " ../train/assets/model=gray_Clipped-elu-L-C40-D-C60-D-F-De128-D-De64-D-De32-D-De16-D-De1-batch_size=40-num_epoch=20-steps_per_epoch=400/weights.h5 \n",
      " ../train/assets/model=gray_Clipped-elu-L-C40-D-C60-D-F-De128-D-De64-D-De32-D-De16-D-De1-batch_size=40-num_epoch=20-steps_per_epoch=400/model=gray_Clipped-elu-L-C40-D-C60-D-F-De128-D-De64-D-De32-D-De16-D-De1-batch_size=40-num_epoch=20-steps_per_epoch=400\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gray_Clipped'\n",
    "batch_size = 40\n",
    "epoch_num = 20\n",
    "steps_per_epoch = 400\n",
    "\n",
    "train_meta_loc = '../train/text/CSV'\n",
    "train_meta_loc = os.path.join(train_meta_loc, 'trainGrayClipped_meta.csv')\n",
    "test_meta_loc = '../test/text/CSV'\n",
    "test_meta_loc = os.path.join(test_meta_loc, 'testGrayClipped_meta.csv')\n",
    "\n",
    "asset_path = '../train/assets'\n",
    "#train_name = f'model={model_name}_C24_C36_C48_D50_C64_C64_De100_De50_De10_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}'\n",
    "#train_name = f'model={model_name}2_elu_C40P_C20P_D25_De128_D30_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}'\n",
    "#train_name = f'model={model_name}_elu_C40_C20_D25_De128_D30_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}__3'\n",
    "#train_name = f'model={model_name}_elu_C40_C20_D35_De128_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}__4'\n",
    "#train_name = f'model={model_name}_elu_C40_C20_De128_De64_De15_De1-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}__2'\n",
    "train_name = f'model={model_name}-elu'\n",
    "model = createCNNModel()\n",
    "model.summary(line_length=100, positions=[0.06,0.55,0.67,0.0], print_fn=a_print_fn)\n",
    "model = 0\n",
    "train_name_end = f'-batch_size={batch_size}-num_epoch={epoch_num}-steps_per_epoch={steps_per_epoch}'\n",
    "train_name = train_name + train_name_end\n",
    "train_path = os.path.join(asset_path, train_name)\n",
    "\n",
    "model_train_meta_loc = os.path.join(train_path, 'train_meta.csv')\n",
    "model_test_meta_loc = os.path.join(train_path, 'test_meta.csv')\n",
    "test_result_path = os.path.join(train_path, 'test.txt')\n",
    "history_loc = os.path.join(train_path, 'history.p')\n",
    "weights_loc = os.path.join(train_path, 'weights.h5')\n",
    "tensorboard_loc = os.path.join(train_path, train_name)\n",
    "print(history_loc, '\\n', weights_loc, '\\n', tensorboard_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    global train_meta_loc, batch_size, epoch_num, steps_per_epoch, train_path\n",
    "    train_meta = pd.read_csv(train_meta_loc)\n",
    "    \n",
    "    data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "    dataset_frames_num = train_meta.shape[0]\n",
    "    train_frames_num = int(.8*(dataset_frames_num-1))\n",
    "    val_frames_num = int((dataset_frames_num - train_frames_num))\n",
    "    print(train_frames_num, val_frames_num)\n",
    "    \n",
    "    \n",
    "    train_index = data_index[:train_frames_num]\n",
    "    val_index = data_index[train_frames_num:(train_frames_num+val_frames_num)]\n",
    "    print(train_index.shape, val_index.shape)\n",
    "    \n",
    "    if not os.path.exists(train_path):\n",
    "        os.makedirs(train_path)\n",
    "    \n",
    "    valid_generator = getValData(train_meta, val_index, batch_size, True)\n",
    "    val_size = len(val_index)\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                                  patience=1, \n",
    "                                  verbose=1, \n",
    "                                  min_delta = 0.23,\n",
    "                                  mode='min',)\n",
    "    \n",
    "    modelCheckpoint = ModelCheckpoint(weights_loc, \n",
    "                                      monitor = 'val_loss', \n",
    "                                      save_best_only = True, \n",
    "                                      mode = 'min', \n",
    "                                      verbose = 1,\n",
    "                                     save_weights_only = True)\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=tensorboard_loc, histogram_freq=0,\n",
    "                                write_graph=True, write_images=True)\n",
    "    #callbacks_list = [modelCheckpoint, tensorboard, earlyStopping]\n",
    "    callbacks_list = [modelCheckpoint, tensorboard]\n",
    "    \n",
    "    model = createCNNModel()\n",
    "    \n",
    "    train_size = len(train_index)\n",
    "    train_generator = getTrainingData(train_meta, train_index, batch_size)\n",
    "    history = model.fit_generator(\n",
    "            train_generator, \n",
    "            steps_per_epoch = steps_per_epoch, \n",
    "            epochs = epoch_num,\n",
    "            callbacks = callbacks_list,\n",
    "            verbose = 1,\n",
    "            validation_data = valid_generator,\n",
    "            validation_steps = val_size)\n",
    "\n",
    "    print(history)\n",
    "    pickle.dump(history.history, open(history_loc, \"wb\"))\n",
    "    \n",
    "    return model, history.history, valid_generator, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16319 4081\n",
      "(16319,) (4080,)\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 421s 1s/step - loss: 262.5074 - accuracy: 1.2500e-04 - val_loss: 723.1339 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 723.13385, saving model to ../train/assets/model=gray_Clipped-elu-L-C40-D-C60-D-F-De128-D-De64-D-De32-D-De16-D-De1-batch_size=40-num_epoch=20-steps_per_epoch=400/weights.h5\n",
      "Epoch 2/20\n",
      " 59/400 [===>..........................] - ETA: 3:54 - loss: 248.7173 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d72e1d971014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-40bcbe47c5ad>\u001b[0m in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTrainingData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     history = model.fit_generator(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \"\"\"\n\u001b[0;32m-> 1718\u001b[0;31m         return training_generator.fit_generator(\n\u001b[0m\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 outs = model.train_on_batch(x, y,\n\u001b[0m\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history, valid_generator, val_size = start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = pickle.load(open(history_loc, \"rb\" ))\n",
    "model = createCNNModel()\n",
    "model.load_weights(weights_loc)\n",
    "# adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# model.compile(optimizer = adam, loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(train_meta_loc)\n",
    "\n",
    "data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "dataset_frames_num = train_meta.shape[0]\n",
    "#train_frames_num = int(.8*(dataset_frames_num-1))\n",
    "train_frames_num = int(.6*(dataset_frames_num-1))\n",
    "val_frames_num = int(.5*((dataset_frames_num-1) - train_frames_num))\n",
    "test_frames_num = int(((dataset_frames_num-1) - (train_frames_num + val_frames_num)))\n",
    "print(train_frames_num, val_frames_num, test_frames_num)\n",
    "\n",
    "train_index = data_index[:train_frames_num]\n",
    "val_index = data_index[train_frames_num:(train_frames_num+val_frames_num)]\n",
    "test_index = data_index[(train_frames_num+val_frames_num):]\n",
    "train_generator = getValData(train_meta, train_index, batch_size, True)\n",
    "valid_generator = getValData(train_meta, val_index, batch_size, True)\n",
    "test_generator = getValData(train_meta, test_index, batch_size, True)\n",
    "train_size = len(train_index)\n",
    "val_size = len(val_index)\n",
    "test_size = len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(train_generator, steps=train_size)\n",
    "print('train score:', train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score = model.evaluate(valid_generator, steps=val_size)\n",
    "print('val score:', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.evaluate(test_generator, steps=test_size)\n",
    "print('test score:', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train score:', train_score)\n",
    "print('val score:', val_score)\n",
    "print('test score:', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train score:', train_score)\n",
    "print('val score:', val_score)\n",
    "print('test score:', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pickle.load(open(history_loc, \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'], 'ro')\n",
    "plt.plot(history['val_loss'], 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size)\n",
    "fig_size[0] = 30\n",
    "fig_size[1] = 30\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "fig_size2 = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(train_meta_loc)\n",
    "\n",
    "data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "dataset_frames_num = train_meta.shape[0]\n",
    "train_frames_num = int(.70*(dataset_frames_num-1))\n",
    "\n",
    "train_index = data_index[:train_frames_num]\n",
    "val_index = data_index[train_frames_num:]\n",
    "print(train_index, val_index)\n",
    "plt.plot(train_meta['speed'][train_index], 'ro')\n",
    "plt.plot(train_meta['speed'][val_index], 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSpeed(a_meta, a_meta_loc, getSpeed, setError):\n",
    "    global model_train_meta_loc, model_test_meta_loc, batch_size, weights_loc\n",
    "\n",
    "    dataset_frames_num = a_meta.shape[0]\n",
    "    print(dataset_frames_num)\n",
    "    \n",
    "    data_index = np.arange(1, (dataset_frames_num))\n",
    "    data_length = data_index.shape[0]\n",
    "    print(data_length)\n",
    "    \n",
    "    data_gen = getValData(a_meta, data_index, batch_size, getSpeed)\n",
    "    \n",
    "    model = createCNNModel()\n",
    "    model.load_weights(weights_loc)\n",
    "    \n",
    "    predicted_speed = model.predict(data_gen, steps=data_length, verbose=1)\n",
    "    predicted_speed.shape = dataset_frames_num-1\n",
    "    \n",
    "    a_meta = a_meta.assign(predicted_speed=pd.Series(np.empty((dataset_frames_num))).values)\n",
    "    a_meta.loc[:, 'predicted_speed'] = np.nan\n",
    "    a_meta.loc[1:,'predicted_speed'] = predicted_speed\n",
    "    \n",
    "    if setError:\n",
    "        a_meta = a_meta.assign(error=pd.Series(np.empty((dataset_frames_num))).values)\n",
    "        a_meta.loc[:, 'error'] = np.nan\n",
    "        a_meta.loc[1:,'error'] = abs(a_meta.loc[1:, 'speed'] - predicted_speed)\n",
    "\n",
    "    a_meta.to_csv(a_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSpeed(train_meta, model_train_meta_loc, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(model_train_meta_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'predicted_speed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'predicted_speed2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'predicted_speed3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(train_meta[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_meta = train_meta.loc[:, :'error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_meta.loc[:,'error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_meta.loc[:,'med_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_meta.loc[:,'mean_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_meta.loc[20376:, 'predicted_speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 50 # was 50\n",
    "#train_meta = train_meta.assign(med_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:, 'med_prd_spd'] = np.nan\n",
    "train_meta.loc[1:,'med_prd_spd'] = train_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).median()\n",
    "train_meta.loc[1:25, 'med_prd_spd'] = train_meta.loc[1:25, 'predicted_speed']\n",
    "train_meta.loc[20376:, 'med_prd_spd'] = train_meta.loc[20376:, 'predicted_speed']\n",
    "#train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 50 # was 50\n",
    "#train_meta = train_meta.assign(mean_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:, 'mean_prd_spd'] = np.nan\n",
    "train_meta.loc[1:,'mean_prd_spd'] = train_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).mean()\n",
    "train_meta.loc[1:25, 'mean_prd_spd'] = train_meta.loc[1:25, 'predicted_speed']\n",
    "train_meta.loc[20376:, 'mean_prd_spd'] = train_meta.loc[20376:, 'predicted_speed']\n",
    "#train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta = train_meta.assign(med_error=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:]['med_error'] = np.nan\n",
    "train_meta.loc[1:,'med_error'] = abs(train_meta.loc[1:, 'speed'] - train_meta.loc[1:,'med_prd_spd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_meta = train_meta.assign(mean_error=pd.Series(np.empty((20400))).values)\n",
    "train_meta.loc[:]['mean_error'] = np.nan\n",
    "train_meta.loc[1:,'mean_error'] = abs(train_meta.loc[1:, 'speed'] - train_meta.loc[1:,'mean_prd_spd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.to_csv(model_train_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(train_meta.loc[:,'error']))\n",
    "print(np.mean(train_meta.loc[:,'med_error']))\n",
    "print(np.mean(train_meta.loc[:,'mean_error']))\n",
    "print()\n",
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'predicted_speed']))\n",
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'med_prd_spd']))\n",
    "print(mean_squared_error(train_meta.loc[1:, 'speed'], train_meta.loc[1:, 'mean_prd_spd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = np.loadtxt('../train/text/train_val_index.txt', np.dtype('int16'), delimiter='\\n')\n",
    "dataset_frames_num = train_meta.shape[0]\n",
    "#train_frames_num = int(.8*(dataset_frames_num-1))\n",
    "train_frames_num = int(.6*(dataset_frames_num-1))\n",
    "val_frames_num = int(.5*((dataset_frames_num-1) - train_frames_num))\n",
    "test_frames_num = int(((dataset_frames_num-1) - (train_frames_num + val_frames_num)))\n",
    "print(train_frames_num, val_frames_num, test_frames_num, train_frames_num+val_frames_num+test_frames_num)\n",
    "\n",
    "train_index = data_index[:train_frames_num]\n",
    "val_index = data_index[train_frames_num:(train_frames_num+val_frames_num)]\n",
    "test_index = data_index[(train_frames_num+val_frames_num):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = 0\n",
    "test2 = 0\n",
    "test3 = 0\n",
    "for i in train_index:\n",
    "    test1 = test1 + train_meta.loc[i, 'error']\n",
    "test1_loss = test1/train_frames_num\n",
    "for i in val_index:\n",
    "    test2 = test2 + train_meta.loc[i, 'error']\n",
    "test2_loss = test2/val_frames_num\n",
    "for i in test_index:\n",
    "    test3 = test3 + train_meta.loc[i, 'error']\n",
    "test3_loss = test3/test_frames_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test1_loss, test2_loss, test3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = pd.read_csv(test_meta_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSpeed(test_meta, model_test_meta_loc, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = pd.read_csv(model_test_meta_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(test_meta[1080:1640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 50 # was 50\n",
    "#test_meta = test_meta.assign(med_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "test_meta.loc[:, 'med_prd_spd'] = np.nan\n",
    "test_meta.loc[1:,'med_prd_spd'] = test_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).median()\n",
    "test_meta.loc[1:25, 'med_prd_spd'] = test_meta.loc[1:25, 'predicted_speed']\n",
    "test_meta.loc[10774:, 'med_prd_spd'] = test_meta.loc[10774:, 'predicted_speed']\n",
    "#test_meta.to_csv(model_test_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_range = 50 # was 50\n",
    "#test_meta = test_meta.assign(mean_prd_spd=pd.Series(np.empty((20400))).values)\n",
    "test_meta.loc[:, 'mean_prd_spd'] = np.nan\n",
    "test_meta.loc[1:,'mean_prd_spd'] = test_meta[1:]['predicted_speed'].rolling(weighted_range, center=True).mean()\n",
    "test_meta.loc[1:25, 'mean_prd_spd'] = test_meta.loc[1:25, 'predicted_speed']\n",
    "test_meta.loc[10774:, 'mean_prd_spd'] = test_meta.loc[10774:, 'predicted_speed']\n",
    "#test_meta.to_csv(model_test_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta.to_csv(model_test_meta_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_length = test_meta.loc[1080:1640].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = 0\n",
    "test2 = 0\n",
    "test3 = 0\n",
    "for i in test_meta.loc[1080:1640, 'predicted_speed']:\n",
    "    test1 = test1 + i\n",
    "test1_loss = test1/stop_length\n",
    "for i in test_meta.loc[1080:1640, 'med_prd_spd']:\n",
    "    test2 = test2 + i\n",
    "test2_loss = test2/stop_length\n",
    "for i in test_meta.loc[1080:1640, 'mean_prd_spd']:\n",
    "    test3 = test3 + i\n",
    "test3_loss = test3/stop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test1_loss, test2_loss, test3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test_meta.loc[:, 'med_prd_spd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv(test_result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size)\n",
    "fig_size[0] = 15\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "fig_size2 = plt.rcParams[\"figure.figsize\"]\n",
    "print(fig_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'predicted_speed'], 'bx')\n",
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'speed'], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'med_prd_spd'], 'bx')\n",
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'speed'], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'mean_prd_spd'], 'bx')\n",
    "plt.plot(train_meta.loc[1:, 'image_index'], train_meta.loc[1:, 'speed'], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_meta.loc[1:, 'image_index'], test_meta.loc[1:, 'predicted_speed'], 'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_meta.loc[1:, 'image_index'], test_meta.loc[1:, 'med_prd_spd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_meta.loc[1:, 'image_index'], test_meta.loc[1:, 'mean_prd_spd'], 'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
